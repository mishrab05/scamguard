{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3340e446-a909-4e08-a6a8-376fd7b8aeeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e2bf7998-423e-407c-a850-2095968e448a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\sreekala\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (3.8.1)\n",
      "Requirement already satisfied: joblib in c:\\users\\sreekala\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from nltk) (1.4.0)\n",
      "Requirement already satisfied: tqdm in c:\\users\\sreekala\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from nltk) (4.65.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\sreekala\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from nltk) (2023.5.5)\n",
      "Requirement already satisfied: click in c:\\users\\sreekala\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from nltk) (8.1.3)\n",
      "Requirement already satisfied: colorama in c:\\users\\sreekala\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from click->nltk) (0.4.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 22.0.4; however, version 24.0 is available.\n",
      "You should consider upgrading via the 'c:\\Users\\SREEKALA\\AppData\\Local\\Programs\\Python\\Python39\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "%pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "05ff4e27-5a45-4efe-959c-21d84bf66606",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\SREEKALA\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import re\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3335492d-4904-4593-b888-1277d232307b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\SREEKALA\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\SREEKALA\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fae5cf7",
   "metadata": {},
   "source": [
    "## Loading the new smishing data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6ab16328-c028-4bca-99a2-b6fc5bf7912c",
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = 'analysisdataset.csv'\n",
    "smishdata = pd.read_csv(filepath, encoding = 'ISO-8859-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "e06fb5fa-3ad3-4bb7-b38f-22a9a78382cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   messageid                                           Fulltext  \\\n",
      "0          3  Text Message\\nThu, Jul 29, 19:10\\nCostco: Dani...   \n",
      "1          5  <\\n+1 (872) 279-0672 >\\nText Message\\nWed, Feb...   \n",
      "2          6  <\\n+1 (806) 224-7886 >\\nText Message\\nThu, Sep...   \n",
      "3          7  Text Message\\nToday 2:30 PM\\nwho played golf\\n...   \n",
      "4          8  (8\\n+1 (775) 537-4497\\ncanador to them\\nTato m...   \n",
      "\n",
      "              Sender    SenderType          timeReceived  \\\n",
      "0              42003    Short Code  03/31/2022, 21:58:50   \n",
      "1  +1 (872) 279-0672  Phone Number  04/02/2022, 02:59:56   \n",
      "2  +1 (806) 224-7886  Phone Number  04/02/2022, 03:03:00   \n",
      "3                NaN           NaN  04/02/2022, 23:42:38   \n",
      "4  +1 (775) 537-4497  Phone Number  04/02/2022, 23:46:34   \n",
      "\n",
      "                                            MainText                    Url  \\\n",
      "0  Costco: Daniel, the code 42003 printed on your...  f2gpy.info/RzNKEwsZve   \n",
      "1  Hi, you still owe UPS $4.10 USD in customs fee...                    NaN   \n",
      "2  wel01.us/r/rest05 WELLS FARGO(CS):Profile lock...      wel01.us/r/rest05   \n",
      "3  Hi, are you who played golf together last time...                    NaN   \n",
      "4  (8 Hi Julianne long time no see, I'm Aleen, ho...                    NaN   \n",
      "\n",
      "  Subdomain Domain   TLD  ... Phishing  Suspicious  Malware        Brand  \\\n",
      "0       NaN  f2gpy  info  ...      0.0         0.0      0.0       Costco   \n",
      "1       NaN    NaN   NaN  ...      NaN         NaN      NaN          UPS   \n",
      "2       NaN  wel01    us  ...      0.0         0.0      0.0  WELLS FARGO   \n",
      "3       NaN    NaN   NaN  ...      NaN         NaN      NaN          NaN   \n",
      "4       NaN    NaN   NaN  ...      NaN         NaN      NaN          NaN   \n",
      "\n",
      "   URL Subcategory         Message Categories FullyQualifiedDomain  \\\n",
      "0    Random Domain              Prize/Contest           f2gpy.info   \n",
      "1              NaN                   Delivery                  NaN   \n",
      "2    Random Domain              Account Alert             wel01.us   \n",
      "3              NaN  Wrong Number/Romance Scam                  NaN   \n",
      "4              NaN  Wrong Number/Romance Scam                  NaN   \n",
      "\n",
      "  Domain Registrar Domain Creation Date Domain Last Update  \n",
      "0  NameCheap, Inc.              7/28/21            7/31/21  \n",
      "1              NaN                  NaN                NaN  \n",
      "2  NameCheap, Inc.              8/30/21            8/30/21  \n",
      "3              NaN                  NaN                NaN  \n",
      "4              NaN                  NaN                NaN  \n",
      "\n",
      "[5 rows x 23 columns]\n"
     ]
    }
   ],
   "source": [
    "print(smishdata.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9833744c-7efc-4e28-a2a7-befc34f58c94",
   "metadata": {},
   "source": [
    "# EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "03362fc0-fd6d-4721-9cba-51bb93cc8f23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Columns to be dropped\n",
    "columns_to_drop = [\n",
    "    'URL Subcategory',\n",
    "    'FullyQualifiedDomain',\n",
    "    'Domain Registrar',\n",
    "    'Domain Creation Date',\n",
    "    'Domain Last Update',\n",
    "    'timeReceived',\n",
    "    'Subdomain',\n",
    "    'RedirectedURL',\n",
    "    'Url',\n",
    "    'Fulltext','messageid','Sender','SenderType','Domain','TLD','Brand','Message Categories'\n",
    "]\n",
    "\n",
    "# Drop the specified columns\n",
    "smishdata.drop(columns_to_drop, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "85fa071b-4819-4261-ad60-e9b68b914ad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove rows with null values\n",
    "smishdata.dropna(inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e8084ef9-651c-4c3f-b6d3-c55280470289",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                             MainText  Detected  Malicious  \\\n",
      "0   Costco: Daniel, the code 42003 printed on your...       0.0        0.0   \n",
      "2   wel01.us/r/rest05 WELLS FARGO(CS):Profile lock...       0.0        0.0   \n",
      "6   Dear. You are invited to join the (Bitcoin) in...       0.0        0.0   \n",
      "7   CITI: Unauthorized activity was detected, we h...       0.0        0.0   \n",
      "10  BofA:Your account has been restricted, visit s...       0.0        0.0   \n",
      "\n",
      "    Phishing  Suspicious  Malware  \n",
      "0        0.0         0.0      0.0  \n",
      "2        0.0         0.0      0.0  \n",
      "6        0.0         0.0      0.0  \n",
      "7        0.0         0.0      0.0  \n",
      "10       0.0         0.0      0.0  \n"
     ]
    }
   ],
   "source": [
    "# Display the first few rows to verify the columns are removed\n",
    "print(smishdata.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ac7e6bdf-6e13-438d-82b6-838d08ef3b1b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Phishing': Phishing\n",
       " 0.0     559\n",
       " 1.0     122\n",
       " 2.0      42\n",
       " 3.0      30\n",
       " 14.0     29\n",
       " 13.0     21\n",
       " 4.0      19\n",
       " 5.0      18\n",
       " 10.0     16\n",
       " 6.0      13\n",
       " 15.0     13\n",
       " 9.0      12\n",
       " 7.0      12\n",
       " 17.0      8\n",
       " 8.0       8\n",
       " 11.0      4\n",
       " 12.0      2\n",
       " 16.0      1\n",
       " 18.0      1\n",
       " Name: count, dtype: int64,\n",
       " 'Suspicious': Suspicious\n",
       " 0.0    850\n",
       " 1.0     78\n",
       " 2.0      2\n",
       " Name: count, dtype: int64,\n",
       " 'Malware': Malware\n",
       " 0.0    805\n",
       " 1.0     67\n",
       " 3.0     27\n",
       " 2.0     24\n",
       " 5.0      5\n",
       " 6.0      1\n",
       " 4.0      1\n",
       " Name: count, dtype: int64}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Figuring out use for phishing, suspicious and malware columns\n",
    "\n",
    "columns_of_interest = ['Phishing', 'Suspicious', 'Malware']\n",
    "\n",
    "# Checking for unique values and their counts in the columns of interest\n",
    "unique_values_counts = {col: smishdata[col].value_counts() for col in columns_of_interest}\n",
    "unique_values_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c983d75f-8bb6-4612-9396-33bb025cf879",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "IsScam\n",
       "False    511\n",
       "True     419\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Scam messages as those that have a value of 1 or more in any of the Phishing, Suspicious, or Malware columns\n",
    "smishdata['IsScam'] = (smishdata['Phishing'] > 0) | (smishdata['Suspicious'] > 0) | (smishdata['Malware'] > 0)\n",
    "\n",
    "\n",
    "\n",
    "# Calculate the count of scam and non-scam messages\n",
    "scam_counts = smishdata['IsScam'].value_counts()\n",
    "\n",
    "scam_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec91fa06-deb0-48c8-b93d-e5fea25d7a98",
   "metadata": {},
   "source": [
    "NON SCAM MESSAGES - 511\n",
    "\n",
    "SCAM MESSAGES - 419"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "64750ecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_drop2 =['Phishing','Suspicious','Malware','Detected','Malicious']\n",
    "smishdata.drop(columns_to_drop2, axis=1, inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5794d14-6710-48ff-a169-5f4cc83011c9",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cd3a868a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the lemmatizer and stopwords list\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "80d1984a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_lemmatize_remove_stopwords(text):\n",
    "    # Convert text to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Remove special characters and punctuation using regex\n",
    "    cleaned_text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    \n",
    "    # Tokenize the cleaned text\n",
    "    word_tokens = word_tokenize(cleaned_text)\n",
    "    \n",
    "    # Filter out the stopwords and lemmatize the remaining words\n",
    "    filtered_lemmatized_text = [lemmatizer.lemmatize(word) for word in word_tokens if word not in stop_words]\n",
    "    \n",
    "    # Rejoin words into a string and return\n",
    "    return ' '.join(filtered_lemmatized_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9625283e",
   "metadata": {},
   "outputs": [],
   "source": [
    "smishdata['MainText'] = smishdata['MainText'].apply(clean_lemmatize_remove_stopwords)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "fd53ea1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MainText</th>\n",
       "      <th>IsScam</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>costco daniel code 42003 printed receipt 10 ca...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>wel01usrrest05 well fargocsprofile locked unus...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>dear invited join bitcoin internal discussion ...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>citi unauthorized activity detected moved fowa...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>bofayour account restricted visit secbofapagel...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             MainText  IsScam\n",
       "0   costco daniel code 42003 printed receipt 10 ca...   False\n",
       "2   wel01usrrest05 well fargocsprofile locked unus...   False\n",
       "6   dear invited join bitcoin internal discussion ...   False\n",
       "7   citi unauthorized activity detected moved fowa...   False\n",
       "10  bofayour account restricted visit secbofapagel...   False"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "smishdata.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6daea203",
   "metadata": {},
   "source": [
    "### Let's now load the UCI data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "36f92e19",
   "metadata": {},
   "outputs": [],
   "source": [
    "sms_text = pd.read_csv(\"spam.csv\", encoding='latin-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "62061bc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "sms_text.dropna(how=\"any\", inplace=True, axis=1)\n",
    "sms_text.columns = ['label', 'message']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "a375d35d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label                                            message\n",
       "0   ham  Go until jurong point, crazy.. Available only ...\n",
       "1   ham                      Ok lar... Joking wif u oni...\n",
       "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3   ham  U dun say so early hor... U c already then say...\n",
       "4   ham  Nah I don't think he goes to usf, he lives aro..."
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sms_text.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8a55bcf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "sms_text['cleaned_message'] = sms_text['message'].apply(clean_lemmatize_remove_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "05b8af8c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'go jurong point crazy available bugis n great world la e buffet cine got amore wat'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sms_text['cleaned_message'].iloc[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cbe64f3-64e7-4c55-95f0-5a4abe87bbc8",
   "metadata": {},
   "source": [
    "## Label Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "44d9969a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "le = LabelEncoder()\n",
    "sms_text['label_encoded'] = le.fit_transform(sms_text['label'])\n",
    "smishdata['IsScam_encoded'] = le.fit_transform(smishdata['IsScam'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "13ce0e5f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>message</th>\n",
       "      <th>cleaned_message</th>\n",
       "      <th>label_encoded</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "      <td>go jurong point crazy available bugis n great ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "      <td>ok lar joking wif u oni</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>free entry 2 wkly comp win fa cup final tkts 2...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "      <td>u dun say early hor u c already say</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "      <td>nah dont think go usf life around though</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label                                            message  \\\n",
       "0   ham  Go until jurong point, crazy.. Available only ...   \n",
       "1   ham                      Ok lar... Joking wif u oni...   \n",
       "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...   \n",
       "3   ham  U dun say so early hor... U c already then say...   \n",
       "4   ham  Nah I don't think he goes to usf, he lives aro...   \n",
       "\n",
       "                                     cleaned_message  label_encoded  \n",
       "0  go jurong point crazy available bugis n great ...              0  \n",
       "1                            ok lar joking wif u oni              0  \n",
       "2  free entry 2 wkly comp win fa cup final tkts 2...              1  \n",
       "3                u dun say early hor u c already say              0  \n",
       "4           nah dont think go usf life around though              0  "
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sms_text.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5d7c6456",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "IsScam_encoded\n",
       "0    511\n",
       "1    419\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "smishdata['IsScam_encoded'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "36da8457",
   "metadata": {},
   "outputs": [],
   "source": [
    "smishdata.drop('IsScam', axis = 1, inplace=True)\n",
    "sms_text.drop(['message','label'],axis = 1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "300547e1",
   "metadata": {},
   "source": [
    "Renaming the column names before concatenation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "805612f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "smishdata.rename(columns={'MainText': 'sms', 'IsScam_encoded': 'label'}, inplace=True)\n",
    "sms_text.rename(columns={'cleaned_message': 'sms', 'label_encoded': 'label'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "35f8fe88",
   "metadata": {},
   "outputs": [],
   "source": [
    "sms_df =  pd.concat([smishdata, sms_text], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "93f6ed34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label\n",
       "0    5336\n",
       "1    1166\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sms_df['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "88522bfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Available: True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Check if CUDA is available\n",
    "cuda_available = torch.cuda.is_available()\n",
    "print(\"CUDA Available:\", cuda_available)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "404d9695",
   "metadata": {},
   "source": [
    "### BERT Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "aed72f78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "  Downloading transformers-4.40.1-py3-none-any.whl (9.0 MB)\n",
      "     ---------------------------------------- 9.0/9.0 MB 5.2 MB/s eta 0:00:00\n",
      "Collecting pyyaml>=5.1\n",
      "  Downloading PyYAML-6.0.1-cp39-cp39-win_amd64.whl (152 kB)\n",
      "     -------------------------------------- 152.8/152.8 KB 8.9 MB/s eta 0:00:00\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\sreekala\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from transformers) (1.26.4)\n",
      "Collecting huggingface-hub<1.0,>=0.19.3\n",
      "  Downloading huggingface_hub-0.22.2-py3-none-any.whl (388 kB)\n",
      "     -------------------------------------- 388.9/388.9 KB 6.0 MB/s eta 0:00:00\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\sreekala\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from transformers) (2023.5.5)\n",
      "Collecting tokenizers<0.20,>=0.19\n",
      "  Downloading tokenizers-0.19.1-cp39-none-win_amd64.whl (2.2 MB)\n",
      "     ---------------------------------------- 2.2/2.2 MB 4.9 MB/s eta 0:00:00\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\sreekala\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from transformers) (4.65.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\sreekala\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from transformers) (21.3)\n",
      "Requirement already satisfied: filelock in c:\\users\\sreekala\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from transformers) (3.12.3)\n",
      "Collecting safetensors>=0.4.1\n",
      "  Downloading safetensors-0.4.3-cp39-none-win_amd64.whl (287 kB)\n",
      "     -------------------------------------- 287.9/287.9 KB 5.9 MB/s eta 0:00:00\n",
      "Requirement already satisfied: requests in c:\\users\\sreekala\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from transformers) (2.28.2)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\sreekala\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (2024.3.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\sreekala\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.11.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\sreekala\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from packaging>=20.0->transformers) (3.0.9)\n",
      "Requirement already satisfied: colorama in c:\\users\\sreekala\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\sreekala\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests->transformers) (2022.12.7)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\sreekala\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests->transformers) (3.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\sreekala\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\sreekala\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests->transformers) (1.26.15)\n",
      "Installing collected packages: safetensors, pyyaml, huggingface-hub, tokenizers, transformers\n",
      "Successfully installed huggingface-hub-0.22.2 pyyaml-6.0.1 safetensors-0.4.3 tokenizers-0.19.1 transformers-4.40.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 22.0.4; however, version 24.0 is available.\n",
      "You should consider upgrading via the 'c:\\Users\\SREEKALA\\AppData\\Local\\Programs\\Python\\Python39\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "%pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1a17a800",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\SREEKALA\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BertTokenizer, BertModel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "fffba8db",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\SREEKALA\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\huggingface_hub\\file_download.py:148: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\SREEKALA\\.cache\\huggingface\\hub\\models--bert-base-uncased. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "bert = BertModel.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71c69a5a",
   "metadata": {},
   "source": [
    "### Define Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "94196d9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_len=512):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = str(self.texts[idx])\n",
    "        label = self.labels[idx]\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            return_token_type_ids=False,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt',\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            'text': text,\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'labels': torch.tensor(label, dtype=torch.long)\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4ddf009",
   "metadata": {},
   "source": [
    "### Define RNN Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6098f00a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNClassifier(nn.Module):\n",
    "    def __init__(self, bert_model, hidden_dim, output_dim, n_layers, bidirectional, dropout):\n",
    "        super(RNNClassifier, self).__init__()\n",
    "        self.bert = bert_model\n",
    "        embedding_dim = bert_model.config.to_dict()['hidden_size']\n",
    "        self.rnn = nn.LSTM(\n",
    "            input_size=embedding_dim,\n",
    "            hidden_size=hidden_dim,\n",
    "            num_layers=n_layers,\n",
    "            bidirectional=bidirectional,\n",
    "            dropout=dropout,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.output = nn.Linear(hidden_dim * 2 if bidirectional else hidden_dim, output_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        with torch.no_grad():\n",
    "            embedded = self.bert(input_ids, attention_mask=attention_mask)[0]\n",
    "        _, (hidden, _) = self.rnn(embedded)\n",
    "        if self.rnn.bidirectional:\n",
    "            hidden = self.dropout(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim=1))\n",
    "        else:\n",
    "            hidden = self.dropout(hidden[-1,:,:])\n",
    "        output = self.output(hidden)\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "10bc4662",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using CUDA: NVIDIA GeForce RTX 3050 Ti Laptop GPU\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"Using CUDA:\", torch.cuda.get_device_name(0))\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"CUDA is not available, using CPU.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e7daab3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RNNClassifier(bert, hidden_dim=256, output_dim=2, n_layers=2, bidirectional=True, dropout=0.1)\n",
    "model = model.to(device)\n",
    "#criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a53f43fd",
   "metadata": {},
   "source": [
    "### Train - Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "08c6e202",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "# Extract features and target\n",
    "texts = sms_df['sms'].tolist()\n",
    "labels = sms_df['label'].tolist()\n",
    "\n",
    "# Split data into training and test sets\n",
    "train_texts, test_texts, train_labels, test_labels = train_test_split(\n",
    "    texts, labels, test_size=0.30, random_state=42\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "511782a8",
   "metadata": {},
   "source": [
    "### Create DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "3f3af2b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Dataset objects\n",
    "train_dataset = TextDataset(train_texts, train_labels, tokenizer)\n",
    "test_dataset = TextDataset(test_texts, test_labels, tokenizer)\n",
    "\n",
    "# Create Data Loaders\n",
    "batch_size = 16\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "61522bba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, criterion, optimizer, dataloader, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch in dataloader:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(input_ids, attention_mask)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    return total_loss / len(dataloader)\n",
    "\n",
    "def evaluate(model, criterion, dataloader, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    total_correct = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "\n",
    "            outputs = model(input_ids, attention_mask)\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_loss += loss.item()\n",
    "            preds = torch.argmax(outputs, dim=1)\n",
    "            total_correct += (preds == labels).sum().item()\n",
    "    return total_loss / len(dataloader), total_correct / len(dataloader.dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "6373f860",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Train Loss: 0.1876, Test Loss: 0.1749, Test Accuracy: 0.93\n",
      "Epoch 2, Train Loss: 0.1517, Test Loss: 0.1530, Test Accuracy: 0.93\n",
      "Epoch 3, Train Loss: 0.1252, Test Loss: 0.1755, Test Accuracy: 0.93\n",
      "Epoch 4, Train Loss: 0.1084, Test Loss: 0.1397, Test Accuracy: 0.93\n"
     ]
    }
   ],
   "source": [
    "# Parameters\n",
    "epochs = 4\n",
    "criterion = nn.CrossEntropyLoss().to(device)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    train_loss = train(model, criterion, optimizer, train_loader, device)\n",
    "    test_loss, test_accuracy = evaluate(model, criterion, test_loader, device)\n",
    "    print(f'Epoch {epoch+1}, Train Loss: {train_loss:.4f}, Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.2f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "fe4f0599",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'model_state_dict.pth')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
